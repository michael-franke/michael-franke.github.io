---
title: "Bayesian data analysis"
subtitle: "Ideas, practices & tools"
author: "Michael Franke & Fabian Dablander"
runtime: shiny
output:
  ioslides_presentation:
    css: mistyle.css
    smaller: yes
    transition: faster
---
```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, dev.args = list(bg = 'transparent'), fig.align='center')
require('ggplot2')
require('reshape2')
require('coda')
require('ggmcmc')
require('rjags')
require('runjags')
require('dplyr')
require('gridExtra')
require('rstan')
theme_set(theme_bw() + theme(plot.background=element_blank()) )

HDIofICDF = function( ICDFname , credMass=0.95 , tol=1e-8 , ... ) {
  # Arguments:
  #   ICDFname is R's name for the inverse cumulative density function
  #     of the distribution.
  #   credMass is the desired mass of the HDI region.
  #   tol is passed to R's optimize function.
  # Return value:
  #   Highest density iterval (HDI) limits in a vector.
  # Example of use: For determining HDI of a beta(30,12) distribution, type
  #   HDIofICDF( qbeta , shape1 = 30 , shape2 = 12 )
  #   Notice that the parameters of the ICDFname must be explicitly named;
  #   e.g., HDIofICDF( qbeta , 30 , 12 ) does not work.
  # Adapted and corrected from Greg Snow's TeachingDemos package.
  incredMass =  1.0 - credMass
  intervalWidth = function( lowTailPr , ICDFname , credMass , ... ) {
    ICDFname( credMass + lowTailPr , ... ) - ICDFname( lowTailPr , ... )
  }
  optInfo = optimize( intervalWidth , c( 0 , incredMass ) , ICDFname=ICDFname ,
                      credMass=credMass , tol=tol , ... )
  HDIlowTailPr = optInfo$minimum
  return( c( ICDFname( HDIlowTailPr , ... ) ,
             ICDFname( credMass + HDIlowTailPr , ... ) ) )
}

pics_folder_path = "//Users/micha/Library/texmf/tex/latex/pics/"
```


## At a glance

- BDA is about what we *should* believe given:
    - some observable data, and
    - our model of how this data was generated.
- Our best friend will be <span style = "color:firebrick">Bayes rule</span>:
     $$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \times \underbrace{P(D \, | \, \theta)}_{likelihood}$$
- If $P(\theta \, | \, D)$ is hard to compute, we resort to ~~magic~~ some clever stuff.

<div align = 'center'>
  <img src="//Users/micha/Library/texmf/tex/latex/pics/Bayes.png" alt="reverendB" style="width: 400px;"/>
</div>

## Example: coin flips

- $\theta \in [0;1]$ is the bias of a coin:
    - if we throw a coin, the outcome will be heads with probability $\theta$
- we have no clue about $\theta$ at the outset:
    - *a priori* we consider every possible value of $\theta$ equally likely
- we observe that 7 out of 24 flips 7 were heads
- what shall we believe about $\theta$ now?

```{r, echo=FALSE, fig.width = 5.5, fig.height = 3, dev.args = list(bg = 'transparent'), fig.align='center'}
  require(ggplot2, quietly = T)
  require(reshape2, quietly = T)
  x = seq(0,1, length.out = 1000)
  prior = dbeta(x, 1, 1)
  posterior = dbeta(x, 8, 18)
  df =data.frame(x = x, prior = prior, posterior = posterior)
  df = melt(df, id.vars = c("x"))
  ggplot(df, aes(x = x, y = value, color = variable)) + geom_line() + xlab("theta") + ylab("our level of credence") + theme(plot.background=element_blank())
```

## "Classical statistics"

- <span style = "color:firebrick">null hypothesis significance testing</span> (NHST)
    - e.g., is the coin fair ($\theta = 0.5$)
- signals if the NH should be rejected
    - not: how likely it is or if it is to be accepted
- relies on sampling distributions & p-values
    - standard "tests" can have rigid built-in assumptions
    - implicitly rely on experimenter's intentions
- looks at point estimates only

<div align = 'center'>
  <img src="//Users/micha/Library/texmf/tex/latex/pics/significance.gif" alt="significance" style="width: 300px;"/>
</div>

## Pros & Cons of BDA {.columns-2} 

### <span style = "color:firebrick">Pro</span>

- well-founded & totally general
- easily extensible / customizable
- more informative / insightful

<img src="https://metrouk2.files.wordpress.com/2014/02/10667_front_11.jpg" alt="Drawing" style="width: 350px;"/>

### <span style = "color:firebrick">Con</span>

- less ready-made, thinking required
- not yet fully digested by community
- higher computational complexity

<div style="text-align: center">
<img src="//Users/micha/Desktop/data/svn/ProComPrag/teachings/bda+cm2015/slides/pics/minorthreat.jpg" alt="Drawing2" style="width: 210px;"/>
</div>


## 3 times Bayes


<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">Bayesian data analysis</span>
    - Bayesian analogues or alternatives to "classical" tests
    
<span style = "color:white"> &nbsp; </span>


<span style = "color:firebrick">Bayesian (cognitive) modeling</span>
    - custom models of the data-generating process
    
<span style = "color:white"> &nbsp; </span>    
    
<span style = "color:firebrick">Bayes in the head</span>
    - model (human) cognition as Bayesian inference

## Goals of this course

<span style = "color:white"> &nbsp; </span>

- to understand basic ideas of BDA (contrast with NHST)

<span style = "color:white"> &nbsp; </span>


- to be able to read current literature on BDA
    - [further reading suggestions on BDA](http://michael-franke.github.io/KogWis2016_bda_tutorial/resources.html)

<span style = "color:white"> &nbsp; </span>

- to be able to start using tools for BDA
    - [pointers to useful tools for BDA](http://michael-franke.github.io/KogWis2016_bda_tutorial/tools.html)

<span style = "color:white"> &nbsp; </span>

- to see how BDA blends seamlessly into cognitive modeling

## The road ahead

<span style = "color:firebrick">theory</span>

- posterior inference & credible values

- Bayes factors & model comparison

- model criticism & Bayesian $p$-values

- Bayesian cousins of $t$-test & regression

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">practice</span>

- basics of MCMC sampling

- tools for BDA ( JAGS, Stan (rstanarm), WebPPL, Jasp )
    
- Bayesian cognitive modeling example


# NHST & $p$-value logic

## binomial distribution

- take $N$ flips of a coin with bias $\theta$
- <span style = "color:firebrick">binomial distribution</span> gives the probability of observing $k$ successes:

$$P(k \mid N, \theta) = {{N}\choose{k}} \, \theta^{k} \, (1-\theta)^{N - k}$$

- example: $N=24$, $\theta = 0.5$

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=2.7}
  
plotData = data.frame(x = 0:24, 
                      y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24),
                       y = dbinom(c(0:7, 17:24), 24, 0.5))
myplot = ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "skyblue", width = 0.35)  + xlab("k") + ylab("P(k | N = 24, theta = 0.5)")
show(myplot)

```

## NHST $p$-value logic

- we observed $k=7$ successes after $N=24$ flips

- <span style = "color:firebrick">null hypothesis</span>: the coin is fair, i.e., $\theta = 0.5$

- the <span style = "color:firebrick">$p$-value</span> of $k=7$ is the probability of observing an outcome that is at least as unlikely as $k=7$ under the NH in infinite repetitions of the experiment

- <span style = "color:firebrick">significance</span>: reject NH if $p$-value is under a predetermined threshold (e.g., 0.05)
    

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=2.7}
  
plotData = data.frame(x = 0:24, 
                      y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24),
                       y = dbinom(c(0:7, 17:24), 24, 0.5))
myplot = ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "skyblue", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "darkblue", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + xlab("k") + ylab("P(k | N = 24, theta = 0.5)") +
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label)) 
  geom_text(x = 3, y = 0.03, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = ""))
show(myplot)

```

## negative binomial distribution

- flip a coin with bias $\theta$ until we have observed $k$ successes
- <span style = "color:firebrick">negative binomial distribution</span> gives the probability of observing $N$ flips:

$$P(N \mid k, \theta) = \frac{z}{N} \, {{N}\choose{k}} \, \theta^{k} \, (1-\theta)^{N - k}$$

- example: $k=7$, $\theta = 0.5$

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=2.7}

negBinom <- function(k, N, theta) {
  sapply(1:length(k), function(x) k[x]/N * dbinom(k[x], N, theta))
}
  
plotData = data.frame(x = 7:35, 
                      y = negBinom(7, 7:35, 0.5))
plotData2 = data.frame(x = 24:35,
                       y = negBinom(7, 24:35, 0.5))
myplotNB = ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "skyblue", width = 0.35) +
  # geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "darkblue", width = 0.35) +
  # geom_hline(yintercept=7/24*dbinom(7,24,0.5)) + xlab("N") + 
  ylab("P(N | k = 7, theta = 0.5)")
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label)) 
  # geom_text(x = 30, y = 0.015, label = paste0("p = " , round(1-sum(negBinom(7, 7:23, 0.5)),3), collapse = ""))
show(myplotNB)

```

## another $p$-value for "same" data set

- we observed $N=24$ flips for a success count of $k=7$
    - NB: same data set as before but obtained differently

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=2.7}

negBinom <- function(k, N, theta) {
  sapply(1:length(k), function(x) k[x]/N * dbinom(k[x], N, theta))
}
  
plotData = data.frame(x = 7:35, 
                      y = negBinom(7, 7:35, 0.5))
plotData2 = data.frame(x = 24:35,
                       y = negBinom(7, 24:35, 0.5))
myplot = ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "skyblue", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "darkblue", width = 0.35) +
  geom_hline(yintercept=7/24*dbinom(7,24,0.5)) + xlab("N") + ylab("P(N | k = 7, theta = 0.5)") +
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label)) 
  geom_text(x = 30, y = 0.015, label = paste0("p = " , round(1-sum(negBinom(7, 7:23, 0.5)),3), collapse = ""))
show(myplot)

```

## some properties of $p$-value NHST

<span style = "color:white"> &nbsp; </span>

- hinges on a what it means to repeat the experiment
    - "model" = data-generating (e.g., psychological) + data-collecting processes

<span style = "color:white"> &nbsp; </span>

- non-significance $\neq$ evidence <span style = "font-style: italic">for</span> "the" alternative hypothesis
    - no information about any alternative hypothesis is used anywhere
    
<span style = "color:white"> &nbsp; </span>

- significance $\neq$ evidence <span style = "font-style: italic">for</span> NH
    - evidence is a relative notion: shifting plausibility between hypotheses

  


# Bayesian basics

## key notions

<span style = "color:firebrick">conditional probability</span>:

$$P(X \, | \, Y) = \frac{P(X \cap Y)}{P(Y)}$$

<span style = "color:firebrick">Bayes rule</span>: 

$$P(X \, | \, Y) = \frac{P(X) \times P(Y \, | \, X)}{P(Y)}$$

<span style = "color:firebrick">Bayes rule for data analysis</span>:

$$\underbrace{P(\theta \, | \, D)}_{posterior} = \frac{\overbrace{P(\theta)}^{prior} \times \overbrace{P(D \, | \, \theta)}^{likelihood}}{\underbrace{P(D)}_{evidence}}$$

## Bayes rule in multi-D

<!---
|  | blond | brown | red | black
:---:|:---:|:---:|:---:|:---:|
blue  | 0.03 | 0.09 | 0.04 | 0.04 |
green | 0.09 | 0.02 | 0 | 0.05 |
brown | 0.09 | 0.41 | 0.01 | 0.13 |
--->

```{r, echo = FALSE, message = FALSE}
  # require('gtools')
  # x = matrix(rdirichlet(1, rep(1, 12)), nrow = 3)
  x = matrix(c(0.03, 0.09, 0.04, 0.04, 0.09, 0.02, 0, 0.05, 0.09, 0.41, 0.01, 0.13), nrow = 3)
  rownames(x) = c("blue", 'green', 'brown')
  colnames(x) = c("blond", 'brown', 'red', 'black')
```

<span style = "color:firebrick">joint probability distribution</span> as two-dimensional matrix:

```{r, echo = FALSE}
  show(round(x,3))
```

<span style = "color:firebrick">marginal distribution</span> over eye color:

```{r, echo = FALSE}
  rowSums(x)
```

<span style = "color:firebrick">conditional probability</span> given black hair:

```{r, echo = FALSE}
  round(x[,4] / sum(x[,4]),2)
```

## model = prior & likelihood

model of a coin flip:

- <span style = "color:firebrick">bias parameter</span> $\theta \in \{0, \frac{1}{3}, \frac{1}{2}, \frac{2}{3}, 1\}$: probability of success on single trial
- flat <span style = "color:firebrick">prior beliefs</span>: $P(\theta) = .2\,, \forall \theta$
- <span style = "color:firebrick">likelihood</span> $P(D \, | \, \theta)$ of data given $\theta$:

```{r, echo = FALSE}
  x = matrix(c(0, 1, 1/3, 2/3, 1/2, 1/2, 2/3, 1/3, 1, 0), nrow = 2)
  rownames(x) = c("heads", 'tails')
  colnames(x) = c("t=0", "t=1/3", "t=1/2", "t=2/3", "t=1")
  show(round(x,2))
```

weighing in $P(\theta)$ gives joint-probability distribution as 2d matrix:

```{r, echo = FALSE}
  y = x * 0.2
  show(round(y,2))
```


## Bayesian inference

<span style = "color:firebrick">Bayes rule</span>: $P(\theta \, | \, D) \propto P(\theta) \times P(D \, | \, \theta)$

```{r, echo = FALSE}
  show(round(y,2))
```


<span style = "color:firebrick">posterior probability</span> $P(\theta \, | \, \text{heads})$ after a toss with heads:

```{r, echo = FALSE}
  posterior = y[1,]/sum(y[1,])
  show(round(posterior,2))
```

```{r, echo = FALSE, fig.width = 5.5, fig.height = 2.75, dev.args = list(bg = 'transparent'), fig.align='center'}
  plotData = data.frame(prior = rep(0.2,5), posterior = posterior, theta = c(0, 1/3, 0.5, 2/3, 1))
  plotData = melt(plotData, id.vars = c("theta") ) %>% rename(distribution = variable)
  myplot = ggplot(plotData, aes(x = theta, y = value, color = distribution )) + geom_point() + geom_line() + theme(plot.background=element_blank()) + ylab("probability")
  show(myplot)
```

## generalized model of coin flips

- inifinite parameter space $\theta \in [0;1]$

- likelihood of observing $k$ successes, given $N$ flips, is <span style = "color:firebrick">binomial distribution</span>:

$$P(k \mid N, \theta) = {{N}\choose{k}} \, \theta^{k} \, (1-\theta)^{N - k}$$

- example of a <span style = "color:firebrick">data-generating model</span>:

<span style = "color:white"> &nbsp; </span>

<div align = 'center'>
  <img src="//Users/micha/Desktop/data/svn/ProComPrag/teachings/bda+cm2015/slides/pics/modelGraphs/basic_binom_withN.jpg" alt="modelGraph" style="width: 400px;"/>
</div>

<div style = "position:absolute; top: 620px; right:60px;">
  [see Lee & Wagenmakers ([2015](https://bayesmodels.com)) on conventions for graphical notation]
</div>


## examples

<span style = "color:firebrick">uniform prior</span>: $\theta \sim Beta(1,1)$

<span style = "color:white"> &nbsp; </span>

```{r, echo = FALSE, fig.width = 5.5, fig.height = 2.75, dev.args = list(bg = 'transparent'), fig.align='center'}
  x = seq(0,1,length.out = 250)
  plotData = data.frame(theta = x,
                        prior = dbeta(x,1,1),
                        k1N1 = dbeta(x,2,1),
                        k1N2 = dbeta(x,2,2),
                        k2N3 = dbeta(x,3,2),
                        k3N4 = dbeta(x,4,2))
  plotData = melt(plotData, id.vars = c("theta")) %>% rename(distribution = variable)
  myplot = ggplot(plotData, aes(x = theta, y = value, color = distribution, linetype = distribution  )) + 
    geom_line(size = 1) + theme(plot.background=element_blank()) + ylab("probability")
  show(myplot)
```


## examples

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">prior biased towards successes</span>: $\theta \sim Beta(7,3)$

```{r, echo = FALSE, fig.width = 5.5, fig.height = 2.75, dev.args = list(bg = 'transparent'), fig.align='center'}
  x = seq(0,1,length.out = 250)
  plotData = data.frame(theta = x,
                        prior = dbeta(x,7,3),
                        k1N1 = dbeta(x,8,3),
                        k1N2 = dbeta(x,8,4),
                        k2N3 = dbeta(x,9,4),
                        k3N4 = dbeta(x,10,4))
  plotData = melt(plotData, id.vars = c("theta")) %>% rename(distribution = variable)
  myplot = ggplot(plotData, aes(x = theta, y = value, color = distribution, linetype = distribution  )) + 
    geom_line(size = 1) + theme(plot.background=element_blank()) + ylab("probability")
  show(myplot)
```

## examples

<span style = "color:firebrick">prior biased towards losses</span>: $\theta \sim Beta(3,7)$

<span style = "color:white"> &nbsp; </span>

```{r, echo = FALSE, fig.width = 5.5, fig.height = 2.75, dev.args = list(bg = 'transparent'), fig.align='center'}
  x = seq(0,1,length.out = 250)
  plotData = data.frame(theta = x,
                        prior = dbeta(x,3,7),
                        k1N1 = dbeta(x,4,7),
                        k1N2 = dbeta(x,4,8),
                        k2N3 = dbeta(x,5,8),
                        k3N4 = dbeta(x,6,8))
  plotData = melt(plotData, id.vars = c("theta")) %>% rename(distribution = variable)
  myplot = ggplot(plotData, aes(x = theta, y = value, color = distribution, linetype = distribution )) + 
    geom_line(size = 1) + theme(plot.background=element_blank()) + ylab("probability")
  show(myplot)
```

# estimation <br> comparison <br> criticism

## outlook

<span style = "color:firebrick">parameter estimation</span>: what to conclude from the data given the model?

- maximum likelihood
- full Bayesian inference
- credible intervals

<span style = "color:firebrick">model comparison</span>: which of several models is better?

- information criteria
- Bayes factors

<span style = "color:firebrick">model criticism</span>: is my model any good?

- $p$-values
    - prior, posterior and classical
- posterior predictive checks

## preview

|    | estimation | comparison | criticism
|:---|:---:|:---:|:---:|
|goal | which $\theta$, given $M$ & $D$? | which better: $M_0$ or $M_1$? | $M$ good model of $D$?
| method | Bayes rule | Bayes factor | $p$-value
|no. of models | 1 | 2 | 1
|$H_0$ | subset of $\theta$  | $P(\theta \mid M_0), P(D \mid \theta, M_0)$ | $P(\theta), P(D \mid \theta)$
|$H_1$ | ---  | $P(\theta \mid M_1), P(D \mid \theta, M_1)$ | ---
| prerequisites | $P(\theta), \alpha \times P(D \mid \theta)$  | --- | test statistic
| pros | lean, easy | intuitive, plausible, Ockham's razor | absolute
| cons | vagueness in ROPE | prior dependence, computational load | sample space?

# estimation

## credible interval

<span style = "color:white"> &nbsp; </span>

For distribution $P(x)$ over $X$, the <span style = "color:firebrick">$n$% credible interval</span> is a subset $Y \subseteq X$ such that:

1. $P(Y) = \frac{n}{100}$, and
2. no point outside of $Y$ is more likely than any point within.

<span style = "color:white"> dummy </span>

<span style = "color:firebrick">Intuition</span>: range of values we are justified to belief in (categorically).

<div style = "position:absolute; top: 620px; right:60px;">
[alternative terminology: highest density interval (HDI), credible region, ...]  
</div>


## examples

<div class = "centered">
<img src="//Users/micha/Desktop/data/svn/ProComPrag/teachings/bda+cm2015/slides/pics/Kruschke_Fig4_5_HDIExamples.png" alt="KruschkeFig5.3" style="width: 370px;"/>
</div>

## posterior credible $\theta$'s

- <span style = "color:firebrick">observation</span>: $k = 7$, $N = 24$
- <span style = "color:firebrick">model</span>:
    - $\theta \sim Beta(1,1)$
    - $P(k) \sim Binomial(k,N)$
- <span style = "color:firebrick">posterior</span>: $P(\theta \mid k=7, N = 24)$

```{r, echo = FALSE, results='hide', warning=FALSE, message=FALSE, fig.width = 5.5, fig.height = 2.75}
plotData = data.frame(theta = seq(0.01,1, by = 0.01),
                      post = dbeta(seq(0.01,1, by = 0.01), 8, 18 ))
hdi = HDIofICDF( qbeta , shape1 = 8 , shape2 = 18 )
hdiData = data.frame(theta = rep(hdi, each = 2),
                     post = c(0,dbeta(hdi, 8, 18), 0))
ggplot(plotData, aes(x = theta, y = post)) + xlim(0,1) + geom_line(color = "black") + ylab("posterior") +
  geom_line(data = hdiData, aes(x = theta, y = post), color = "firebrick", size = 1) +
  geom_text(x = mean(hdi), y = 1, label = "HDI: 0.14 - 0.48")

```

## ROPEs and credible values

<span style = "color:white"> &nbsp; </span>

### <span style = "color:firebrick">regions of practical equivalence (ROPE)</span>

- small regions $[\theta - \epsilon, \theta + \epsilon]$ around each $\theta$
    - values (practically) indistinguishable from $\theta$

<span style = "color:white"> &nbsp; </span>
  
### <span style = "color:firebrick">credible values</span>

- value $\theta$ is <span style = "color:firebrick">rejectable</span> if its ROPE lies entirely outside of posterior HDI
- value $\theta$ is <span style = "color:firebrick">believable</span>
 if its ROPE lies entirely whithin posterior HDI

<div style = "position:absolute; top: 620px; right:60px;">
  [this is mainly Kruschke's ([2014](https://sites.google.com/site/doingbayesiandataanalysis/)) approach]
</div>


## independence of stopping rule

<span style = "color:white"> &nbsp; </span>

- $P(\theta \mid D)$ is independent of stopping criterion during data collection
- any normalizing constant $X$ cancels out:

<span style = "color:white"> &nbsp; </span>

$$
\begin{align*}
P(\theta \mid D) & = \frac{P(\theta) \ P(D \mid \theta)}{\int_{\theta'} P(\theta') \ P(D \mid \theta')} \\
& = \frac{ \frac{1}{X} \ P(\theta) \ P(D \mid \theta)}{ \ \frac{1}{X}\ \int_{\theta'} P(\theta') \ P(D \mid \theta')} \\
& = \frac{P(\theta) \ \frac{1}{X}\ P(D \mid \theta)}{  \int_{\theta'} P(\theta') \ \frac{1}{X}\ P(D \mid \theta')}
\end{align*}
$$

# model comparison

## model comparison

<span style = "color:white"> &nbsp; </span>

### <span style = "color:firebrick">main question</span>

which model is better given data? (e.g., null model vs. alternative)

<span style = "color:white"> &nbsp; </span>

### <span style = "color:firebrick">caveat</span> 

different answers may imply different notions of "model" & purposes of modeling


## key notions


### <span style = "color:firebrick">information criteria</span>

- maximum likelihood estimation
- free parameters
- model complexity
    
    
### <span style = "color:firebrick">Bayes factors</span>

- Savage-Dickey method
- Lindley paradox

## Akaike information criterion

### <span style = "color:firebrick">motivation</span>

- model is better, the higher $P(D \mid \hat{\theta})$
    - where $\hat{\theta} \in \arg \max_\theta P(D \mid \theta)$
- model is worse, the more parameters it has
    - principle of parsimony (Ockham's razor)
- information theoretic notion:
    - amount of information lost when we assume that the data was generated by model under $\hat{\theta}$

### <span style = "color:firebrick">definition</span>

Let $M$ be a model with $k$ parameters, and $D$ be some data:

$$\text{AIC}(M, D) = 2k - \ln P(D \mid \hat{\theta})$$

The smaller the AIC, the better the model.

## model comparison by AIC

compute AICs for both models:

```{r}
k = 7
N = 24
AIC_nh = - dbinom(k,N, prob = 0.5, log = T)
AIC_ah = 2 - dbinom(k,N, prob = k/N, log = T)
show(data.frame(model = c("null", "alt"),
                AIC = c(AIC_nh, AIC_ah) ))
```

<span style = "color:firebrick">weight of evidence</span>:

- let $AIC_i$ be model $i$'s AIC & let $\Delta_i = AIC_i - \min_j AIC_j$
- weight of evidence for model $i$ is: $w_i \propto \exp(-0.5 \Delta_i)$
- e.g., weight of evidence for alternative model is `r round(exp(0.5*(AIC_nh - AIC_ah)),3)` (small!)


<div style = "position:absolute; top: 620px; right:60px;">
  [e.g., Burnham & Anderson (2002)]
</div>



## remarks

- given more and more data, repeated model selection by AIC does not guarantee ending up with the true model
- "model" for AICs is just likelihood; no prior
- discounting number of parameters, like AIC does, does not take effective strength of parameters into account
- there are other information criteria that address some of these problems:
    - Bayesian information criterion
    - deviance information criterion

## Bayes factors

- take two models (in the sense of "model = prior + likelihood")
    - $P(\theta_1 , M_1)$ and $P(D \mid \theta_1, M_1)$
    - $P(\theta_2 , M_2)$ and $P(D \mid \theta_2, M_2)$
- ideally, we'd want to know the <span style = "font-style: italic">absolute probability</span> of $M_i$ given the data
    - but then we'd need to know set of all models (for normalization)
- alternatively, we take odds of models given the data:

$$\underbrace{\frac{P(M_1 \mid D)}{P(M_2 \mid D)}}_{\text{posterior odds}} = \underbrace{\frac{P(D \mid M_1)}{P(D \mid M_2)}}_{\text{Bayes factor}} \ \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{prior odds}}$$

The <span style = "color:firebrick">Bayes factor</span> is the factor by which our prior odds are changed by the data.

## evidence

<span style = "color:firebrick">Bayes factor</span> in favor of model $M_1$

$$\text{BF}(M_1 > M_2) = \frac{P(D \mid M_1)}{P(D \mid M_2)}$$

<span style = "color:firebrick">evidence</span> of model $M_i$ (= marginal likelihood of data)

$$P(D \mid M_i) = \int P(\theta_i , M_i) \ P(D \mid \theta_i, M_i) \text{ d}\theta_i$$

evidence marginalizes out parameters $\theta_i$: function of prior and likelihood

## example

- observed: $k = 7$ out of $N = 24$ flips came up heads
- goal: compare a null-model $M_0$ with an alternative model $M_1$
- model specification:
    - $M_0$ has $\theta = 0.5$ and $k \sim \text{Binomial}(0.5, N)$
    - $M_1$ has $\theta \sim \text{Beta}(1,1)$ and $k \sim \text{Binomial}(\theta, N)$
    
$$
\begin{align*}
\text{BF}(M_0 > M_1) & = \frac{P(D \mid M_0)}{P(D \mid M_1)} \\
  & = \frac{\text{Binomial}(k,N,0.5)}{\int_0^1 \text{Beta}(\theta, 1, 1) \ \text{Binomial}(k,N, \theta) \text{ d}\theta} \\
  & = {{N}\choose{k}} \frac{  0.5^{k} \, (1-0.5)^{N - k}}{\int_0^1 \theta^{k} \, (1-\theta)^{N - k} \text{ d}\theta} \\
  & = \frac{0.5^{k} \, (1-0.5)^{N - k}}{BetaFunction(k+1, N-k+1)} \approx `r round(0.5^7 * 0.5^(24-7) / beta(8, 25-7),3)`
\end{align*}
$$

## how to interpret Bayes factors

BF(M1 > M2) | interpretation
:---:|:---:|
1 | irrelevant data
1 - 3 | hardly worth ink or breath
3 - 6 | anecdotal
6 - 10 | now we're talking: substantial
10 - 30 | strong
30 - 100 | very strong
100 + | decisive (bye, bye $M_2$!)

## how to caculate Bayes factors

1. calculate each model's evidence
    - brute force clever math
    - grid approximation
    - by MCMC estimation
2. calculate Bayes factor:
    - transdimensional MCMC
    - Savage-Dickey method

## properly nested models

- suppose that there are $n$ continuous parameters of interest $\theta = \langle \theta_1, \dots, \theta_n \rangle$
- $M_1$ is a model defined by $P(\theta \mid M_1)$ & $P(D \mid \theta, M_1)$
- $M_0$ is <span style = "color:firebrick">properly nested</span> under $M_1$ if:
    - $M_0$ assigns fixed values to parameters $\theta_i = x_i, \dots, \theta_n = x_n$
    - $\lim_{\theta_i \rightarrow x_i, \dots, \theta_n \rightarrow x_n} P(\theta_1, \dots, \theta_{i-1} \mid \theta_i, \dots, \theta_n, M_1) = P(\theta_1, \dots, \theta_{i-1} \mid M_0)$
    - $P(D \mid \theta_1, \dots, \theta_{i-1}, M_0) = P(D \mid \theta_1, \dots, \theta_{i-1}, \theta_i = x_i, \dots, \theta_n = x_n, M_1)$

## Savage-Dickey method

let $M_0$ be <span style = "color:firebrick">properly nested</span> under $M_1$ s.t. $M_0$ fixes $\theta_i = x_i, \dots, \theta_n = x_n$

$$
\begin{align*}
\text{BF}(M_0 > M_1) & = \frac{P(D \mid M_0)}{P(D \mid M_1)} \\
  & = \frac{P(\theta_i = x_i, \dots, \theta_n = x_n \mid D, M_1)}{P(\theta_i = x_i, \dots, \theta_n = x_n \mid M_1)}
\end{align*}
$$

```{r, echo = FALSE, results='hide', warning=FALSE, message=FALSE, fig.align='center', fig.width=6, fig.height=3}
plotData = data.frame(theta = seq(0.01,1, by = 0.01),
                      posterior = dbeta(seq(0.01,1, by = 0.01), 8, 18 ),
                      prior = dbeta(seq(0.01,1, by = 0.01), 1, 1))
plotData = melt(plotData, measure.vars = c("posterior", "prior"))
pointData = data.frame(x = c(0.5,0.5), y = c(dbeta(0.5,8,18),1))

ggplot(plotData, aes(x = theta, y = value, color = variable)) + xlim(0,1) + geom_line() + ylab("probability") +
  geom_segment(aes(x = 0.52, y = 0, xend = 0.52, yend = 1), color = "darkgray") +
  geom_segment(aes(x = 0.48, y = 0, xend = 0.48, yend = dbeta(0.5,8,18)), color = "darkgray") +
  geom_segment(aes(x = 0.5, y = 1, xend = 0.52, yend = 1), color = "darkgray") +
  geom_segment(aes(x = 0.5, y = dbeta(0.5,8,18), xend = 0.48, yend = dbeta(0.5,8,18)), color = "darkgray") +
  annotate("point", x = 0.5, y = 1, color = "black") +
  annotate("point", x = 0.5, y = dbeta(0.5,8,18), color = "black") + 
  annotate("text", x = 0.3, y = 0.25, color = "darkgray", label = "P(0.5 | D, M1) = 0.516", size = 3) +
  annotate("text", x = 0.68, y = 0.75, color = "darkgray", label = "P(0.5 | M1) = 1", size = 3) +
  theme(legend.title=element_blank())

```

## recap

Bayes rule for <span style = "color:firebrick">parameter estimation</span>:

$$\underbrace{P(\theta \, | \, D)}_{\text{posterior}} \propto \underbrace{P(\theta)}_{\text{prior}} \times \underbrace{P(D \, | \, \theta)}_{\text{likelihood}}$$

Bayes factor for <span style = "color:firebrick">model comparison</span>:

$$
\begin{align*}
\underbrace{\frac{P(M_1 \mid D)}{P(M_2 \mid D)}}_{\text{posterior odds}} & = \underbrace{\frac{P(D \mid M_1)}{P(D \mid M_2)}}_{\text{Bayes factor}} \ \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{prior odds}} \\
\underbrace{P(D \mid M_i)}_{\text{evidence}} & = \int P(\theta_i \mid M_i) \ P(D \mid \theta_i, M_i) \text{ d}\theta_i
\end{align*}
$$

$p$-values for <span style = "color:firebrick">null-hypothesis significance testing</span>

## overview

|    | estimation | comparison | criticism
|:---|:---:|:---:|:---:|
|goal | which $\theta$, given $M$ & $D$? | which better: $M_0$ or $M_1$? | $M$ good model of $D$?
| method | Bayes rule | Bayes factor | $p$-value
|no. of models | 1 | 2 | 1
|$H_0$ | subset of $\theta$  | $P(\theta \mid M_0), P(D \mid \theta, M_0)$ | $P(\theta), P(D \mid \theta)$
|$H_1$ | ---  | $P(\theta \mid M_1), P(D \mid \theta, M_1)$ | ---
| prerequisites | $P(\theta), \alpha \times P(D \mid \theta)$  | --- | test statistic
| pros | lean, easy | intuitive, plausible, Ockham's razor | absolute
| cons | vagueness in ROPE | prior dependence, computational load | sample space?

# comparison of approaches

## Jeffreys-Lindley paradox

```{r}
k = 49581
N = 98451
show(k/N)
```


<div style = "float:left; width:45%;">

### <span style = "color:firebrick">$p$-value NHST</span>

```{r}
binom.test(k, N)$p.value
```
</div>
<div style = "float:right; width:45%;">

### <span style = "color:firebrick">Savage-Dickey BF</span>

```{r}
dbeta(0.5, k+1, N - k + 1)
```  
</div>

## simulation

- let the true bias be $\theta = 0.5$
- generate all possible outcomes $k$ keeping $N$ fixed
    - $N \in \{ 10, 100, 1000, 10000, 100000 \}$
    - true frequency of $k$ is $Binomial(k \mid N, \theta = 0.5)$
- look at the frequency of test results, coded thus:

|    | estimation | comparison | criticism
|:---|:---:|:---:|:---:|
|$M_0$ | $[.5-\epsilon, 0.5+\epsilon] \sqsubseteq$ 95% HDI or v.v. | BF($M_0$>$M_1$) > 6 | $p$ > 0.05
|$M_1$ | $[.5-\epsilon, 0.5+\epsilon] \, \cap \,$ 95% HDI $=\emptyset$ | BF($M_1$>$M_0$) > 6 | $p$ <= 0.05
|?? | otherwise | otherwise | never

## results

```{r, echo = FALSE}
load(file = "results_binomial_comparison.Rdata")
```


Bayes factor model comparison selects $M_0$ correctly with probability `r round(sum(filter(results[[1]], BFdecision == "H0" & N == 10000)$frequency),3)` for $N = 10000$, and with `r round(sum(filter(results[[1]], BFdecision == "H0" & N == 100000)$frequency),3)` for $N = 100000$.

```{r, echo = FALSE, results='hide', warning=FALSE, message=FALSE, fig.align='center', fig.width=6, fig.height=3}

rf = results$result_frequencies
result_freq_plot = ggplot(rf, aes(x = N, y = value)) + scale_x_log10() +
  geom_point(data = filter(rf, rule == "BF"), aes(y = value, shape = decision, color = "BF")) +
  geom_point(data = filter(rf, rule == "p-value"), aes(y = value, shape = decision, color = "p-value")) +
  geom_point(data = filter(rf, rule == "ROPE"), aes(y = value, shape = decision, color = "ROPE")) +
  geom_line(data = filter(rf, rule == "BF"), aes(y = value, group = decision, color = rule)) +
  geom_line(data = filter(rf, rule == "p-value"), aes(y = value, group = decision, color = rule)) +
  geom_line(data = filter(rf, rule == "ROPE"), aes(y = value, group = decision, color = rule)) +
  theme(legend.title=element_blank())
show(result_freq_plot)

```

<div style = "position:absolute; top: 620px; right:60px;">
  [c.f., Lindley's solution to 'paradox': adjust $p$ depending on $N$; similar for ROPE's $\epsilon$]
</div>

# wrap-up

## notions covered

- NHST $p$-value logic

<span style = "color:white"> &nbsp; </span>

- posterior inference
    - credible intervals, ROPE

<span style = "color:white"> &nbsp; </span>

- model comparison
    - Bayes factors
    